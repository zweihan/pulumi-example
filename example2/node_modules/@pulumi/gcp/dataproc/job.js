"use strict";
// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.Job = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * Manages a job resource within a Dataproc cluster within GCE. For more information see
 * [the official dataproc documentation](https://cloud.google.com/dataproc/).
 *
 * !> **Note:** This resource does not support 'update' and changing any attributes will cause the resource to be recreated.
 *
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const mycluster = new gcp.dataproc.Cluster("mycluster", {region: "us-central1"});
 * // Submit an example spark job to a dataproc cluster
 * const spark = new gcp.dataproc.Job("spark", {
 *     region: mycluster.region,
 *     forceDelete: true,
 *     placement: {
 *         clusterName: mycluster.name,
 *     },
 *     sparkConfig: {
 *         mainClass: "org.apache.spark.examples.SparkPi",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         args: ["1000"],
 *         properties: {
 *             "spark.logConf": "true",
 *         },
 *         loggingConfig: {
 *             driverLogLevels: {
 *                 root: "INFO",
 *             },
 *         },
 *     },
 * });
 * // Submit an example pyspark job to a dataproc cluster
 * const pyspark = new gcp.dataproc.Job("pyspark", {
 *     region: mycluster.region,
 *     forceDelete: true,
 *     placement: {
 *         clusterName: mycluster.name,
 *     },
 *     pysparkConfig: {
 *         mainPythonFileUri: "gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py",
 *         properties: {
 *             "spark.logConf": "true",
 *         },
 *     },
 * });
 * export const sparkStatus = spark.statuses.apply(statuses => statuses[0].state);
 * export const pysparkStatus = pyspark.statuses.apply(statuses => statuses[0].state);
 * ```
 *
 * ## Import
 *
 * This resource does not support import.
 */
class Job extends pulumi.CustomResource {
    /**
     * Get an existing Job resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new Job(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of Job.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Job.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["driverControlsFilesUri"] = state ? state.driverControlsFilesUri : undefined;
            resourceInputs["driverOutputResourceUri"] = state ? state.driverOutputResourceUri : undefined;
            resourceInputs["forceDelete"] = state ? state.forceDelete : undefined;
            resourceInputs["hadoopConfig"] = state ? state.hadoopConfig : undefined;
            resourceInputs["hiveConfig"] = state ? state.hiveConfig : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["pigConfig"] = state ? state.pigConfig : undefined;
            resourceInputs["placement"] = state ? state.placement : undefined;
            resourceInputs["prestoConfig"] = state ? state.prestoConfig : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["pysparkConfig"] = state ? state.pysparkConfig : undefined;
            resourceInputs["reference"] = state ? state.reference : undefined;
            resourceInputs["region"] = state ? state.region : undefined;
            resourceInputs["scheduling"] = state ? state.scheduling : undefined;
            resourceInputs["sparkConfig"] = state ? state.sparkConfig : undefined;
            resourceInputs["sparksqlConfig"] = state ? state.sparksqlConfig : undefined;
            resourceInputs["statuses"] = state ? state.statuses : undefined;
        }
        else {
            const args = argsOrState;
            if ((!args || args.placement === undefined) && !opts.urn) {
                throw new Error("Missing required property 'placement'");
            }
            resourceInputs["forceDelete"] = args ? args.forceDelete : undefined;
            resourceInputs["hadoopConfig"] = args ? args.hadoopConfig : undefined;
            resourceInputs["hiveConfig"] = args ? args.hiveConfig : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["pigConfig"] = args ? args.pigConfig : undefined;
            resourceInputs["placement"] = args ? args.placement : undefined;
            resourceInputs["prestoConfig"] = args ? args.prestoConfig : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["pysparkConfig"] = args ? args.pysparkConfig : undefined;
            resourceInputs["reference"] = args ? args.reference : undefined;
            resourceInputs["region"] = args ? args.region : undefined;
            resourceInputs["scheduling"] = args ? args.scheduling : undefined;
            resourceInputs["sparkConfig"] = args ? args.sparkConfig : undefined;
            resourceInputs["sparksqlConfig"] = args ? args.sparksqlConfig : undefined;
            resourceInputs["driverControlsFilesUri"] = undefined /*out*/;
            resourceInputs["driverOutputResourceUri"] = undefined /*out*/;
            resourceInputs["statuses"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Job.__pulumiType, name, resourceInputs, opts);
    }
}
exports.Job = Job;
/** @internal */
Job.__pulumiType = 'gcp:dataproc/job:Job';
//# sourceMappingURL=job.js.map